// lib/vision.ts (YENƒ∞ DOSYA)

import type { Traits } from "./types";

// Hata durumunda veya profil resmi anlamsƒ±zsa kullanƒ±lacak varsayƒ±lan √∂zellikler
const DEFAULT_TRAITS: Traits = {
  description: "a mysterious and cool digital entity",
  main_colors: ["#000000", "#FFFFFF"],
  style: "digital-art",
  accessory: "glowing aura",
};

// Use direct HTTP request to v1 API endpoint (not v1beta)
// This ensures we use the correct API version regardless of package version
export async function analyzeProfileImage(imageUrl: string): Promise<Traits> {
  try {
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) {
      console.error("GEMINI_API_KEY not set");
      return DEFAULT_TRAITS;
    }

    // AI #1'e vereceƒüimiz komut (prompt)
    const prompt = `Analyze this image. Your goal is to extract key visual traits.
    Respond ONLY with a valid JSON object matching this TypeScript type:
    type Traits = {
      description: string; // A 2-3 word description (e.g., 'a person smiling', 'a blue logo', 'a cat')
      main_colors: string[]; // An array of the 3 most dominant hex color codes (e.g., ["#FF0000", "#0000FF"])
      style: string; // The overall style (e.g., 'photographic', 'cartoon', 'anime', 'text-logo', 'pixel-art')
      accessory: string; // The most prominent accessory (e.g., 'glasses', 'hat', 'earring', 'none')
    };
    
    RULES:
    1.  If the image is just text, a logo, or abstract art, BE CREATIVE. Invent traits that fit its vibe.
    2.  Always return valid JSON. Do not write any text outside the JSON block.
    3.  Always fill all fields. Do not use 'unknown' or 'N/A'.
    
    Example for a cat photo:
    {"description": "a fluffy cat", "main_colors": ["#E0B88A", "#3C2A1E", "#FFFFFF"], "style": "photographic", "accessory": "whiskers"}
    
    Example for a text logo:
    {"description": "a tech logo", "main_colors": ["#0A74DA", "#FFFFFF", "#000000"], "style": "text-logo", "accessory": "glowing lines"}`;

    // Resmi URL'den alƒ±p Base64'e √ßevirme
    const imageResponse = await fetch(imageUrl);
    const imageBuffer = await imageResponse.arrayBuffer();
    const imageBase64 = Buffer.from(imageBuffer).toString("base64");
    const mimeType = imageResponse.headers.get("content-type") || "image/jpeg";

    // First, get available models using ListModels endpoint
    let availableModels: string[] = [];
    try {
      const listModelsUrl = `https://generativelanguage.googleapis.com/v1/models?key=${apiKey}`;
      const listResponse = await fetch(listModelsUrl);
      
      if (listResponse.ok) {
        const listData = await listResponse.json();
        availableModels = (listData.models || [])
          .map((m: any) => m.name?.replace("models/", "") || "")
          .filter((name: string) => name && name.includes("gemini"));
        
        console.log("üìã Available Gemini models:", availableModels);
      } else {
        console.warn("‚ö†Ô∏è Could not list models, using fallback list");
      }
    } catch (error) {
      console.warn("‚ö†Ô∏è Error listing models, using fallback list:", error);
    }

    // Fallback model list if ListModels fails
    if (availableModels.length === 0) {
      availableModels = [
        "gemini-1.5-flash-001",
        "gemini-1.5-pro-001",
        "gemini-pro",
        "gemini-1.5-flash",
        "gemini-1.5-pro",
      ];
    }

    // Use v1 API endpoint directly (not v1beta)
    const requestBody = {
      contents: [
        {
          parts: [
            { text: prompt },
            {
              inlineData: {
                data: imageBase64,
                mimeType: mimeType,
              },
            },
          ],
        },
      ],
    };

    let response: Response | null = null;
    let lastError: string | null = null;

    // Try available models in order
    for (const modelName of availableModels) {
      try {
        const apiUrl = `https://generativelanguage.googleapis.com/v1/models/${modelName}:generateContent?key=${apiKey}`;
        
        response = await fetch(apiUrl, {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify(requestBody),
        });

        if (response.ok) {
          console.log(`‚úÖ Successfully used model: ${modelName}`);
          break;
        } else {
          const errorText = await response.text();
          lastError = `Model ${modelName}: ${response.status} ${errorText}`;
          console.warn(`‚ö†Ô∏è Model ${modelName} failed:`, lastError);
          response = null;
        }
      } catch (error) {
        lastError = `Model ${modelName}: ${error instanceof Error ? error.message : "Unknown error"}`;
        console.warn(`‚ö†Ô∏è Model ${modelName} error:`, lastError);
        response = null;
      }
    }

    if (!response || !response.ok) {
      console.error("Gemini API error - all models failed:", lastError);
      console.error("Available models that were tried:", availableModels);
      throw new Error(`Gemini API error: ${lastError || "All models failed"}`);
    }

    const result = await response.json();
    
    // Extract text from response
    const responseText = result.candidates?.[0]?.content?.parts?.[0]?.text;
    if (!responseText) {
      console.error("No text in Gemini response:", JSON.stringify(result));
      return DEFAULT_TRAITS;
    }
    
    // JSON'u temizle ve parse et
    const jsonText = responseText.replace(/```json/g, "").replace(/```/g, "").trim();
    const parsedTraits = JSON.parse(jsonText);

    return parsedTraits;

  } catch (error) {
    console.error("Vision AI error:", error);
    // Hata olursa varsayƒ±lan √∂zellikleri d√∂nd√ºr
    return DEFAULT_TRAITS;
  }
}